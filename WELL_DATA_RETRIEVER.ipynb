{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4_-AartcQrwH",
        "7nsmyCCUVXof",
        "IIavsUR1fZuc",
        "0QuLjp0cV8ns",
        "V1a3UsuhVAgh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_jshS7gHIroP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHAT YOU NEED TO DO:\n",
        "Export the well data from Petrel by right-clicking the wells on the input pane, selecting \"Export,\" then choosing \"Export as well path/deviation.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Since I am writing this code using google collab,\n",
        "on google collab, what you need to do is :\n",
        "\n",
        "create folder : \"wells\"\n",
        "and then put the exported files there, as shown on the image below\n"
      ],
      "metadata": {
        "id": "-DQIflvHJoll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beside that, you need to convert the surface data as points, and export it in Irap Classic Points(ASCII),\n",
        "the upload the file on the google colab upload widget"
      ],
      "metadata": {
        "id": "_7lghrRxSvNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DOWNLOADING NECESSARY LIBRARY**"
      ],
      "metadata": {
        "id": "4_-AartcQrwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT-cYjEF-rsy",
        "outputId": "5fd153bb-ec68-418e-ec08-52874a8eb0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lasio in /usr/local/lib/python3.10/dist-packages (0.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lasio) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install lasio\n",
        "import lasio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "from scipy.interpolate import griddata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROCESSING DATA AND ITS DIRECTORY**"
      ],
      "metadata": {
        "id": "7nsmyCCUVXof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "input_dir = \"/content/wells\"  # Store the directory path\n",
        "output_dir = \"/content/csv_folder\"  # Store the output directory path\n",
        "#create output if no folder available\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# List all files in the folder\n",
        "input_filenames = os.listdir(input_dir)\n",
        "\n",
        "for file in input_filenames:\n",
        "\n",
        "    # Process of turning the files into a CSV\n",
        "    file_path = os.path.join(input_dir, file)  # Construct the full file path\n",
        "\n",
        "    # Ignoring .ipynb files\n",
        "    if os.path.isfile(file_path) and not file.endswith('.ipynb'):  # Edited: Added file type check\n",
        "        with open(file_path, 'r') as infile:\n",
        "            lines = infile.readlines()\n",
        "\n",
        "        # Initialize an empty list to store the data rows\n",
        "        data = []\n",
        "\n",
        "        # Extract the data rows excluding header and footer\n",
        "        try:  # Added: Try block to handle missing separator\n",
        "            start_index = lines.index('#===============================================================================================================================================\\n') + 1\n",
        "        except ValueError:  # Added: Exception handling for missing separator\n",
        "            start_index = 0\n",
        "        end_index = len(lines)  # Edited: Simplified end_index assignment\n",
        "\n",
        "        for line in lines[start_index:end_index]:\n",
        "            if line.strip() and not line.startswith('#'):\n",
        "                row = line.split()\n",
        "                data.append(row)\n",
        "\n",
        "        # Write the extracted data into a CSV file with the same filename +csv\n",
        "        output_filename = file + '.csv' # Edited: Preserve the original filename\n",
        "        output_filepath = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        with open(output_filepath, 'w', newline='') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            csv_writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "rZYiMuln-wsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLUSTER DICT"
      ],
      "metadata": {
        "id": "IIavsUR1fZuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_cluster = {0: {0: '\"A\"',\n",
        "  1: '\"AA\"',\n",
        "  2: '\"AB\"',\n",
        "  3: '\"B\"',\n",
        "  4: '\"BA\"',\n",
        "  5: '\"C\"',\n",
        "  6: '\"C\"',\n",
        "  7: '\"C\"',\n",
        "  8: '\"CA\"',\n",
        "  9: '\"CB\"',\n",
        "  10: '\"CB\"',\n",
        "  11: '\"CB\"',\n",
        "  12: '\"D\"',\n",
        "  13: '\"DA\"',\n",
        "  14: '\"E\"',\n",
        "  15: '\"EA\"',\n",
        "  16: '\"EB\"',\n",
        "  17: '\"F\"',\n",
        "  18: '\"FA\"',\n",
        "  19: '\"G\"',\n",
        "  20: '\"GA\"',\n",
        "  21: '\"GB\"',\n",
        "  22: '\"GC\"',\n",
        "  23: '\"GD\"',\n",
        "  24: '\"H\"',\n",
        "  25: '\"HA\"',\n",
        "  26: '\"HB\"',\n",
        "  27: '\"HC\"',\n",
        "  28: '\"I\"',\n",
        "  29: '\"J\"',\n",
        "  30: '\"K\"',\n",
        "  31: '\"KA\"',\n",
        "  32: '\"L\"',\n",
        "  33: '\"LA\"',\n",
        "  34: '\"LB\"',\n",
        "  35: '\"M\"',\n",
        "  36: '\"N\"',\n",
        "  37: '\"O\"',\n",
        "  38: '\"OA\"',\n",
        "  39: '\"P\"',\n",
        "  40: '\"PA\"',\n",
        "  41: '\"Q\"',\n",
        "  42: '\"Q\"',\n",
        "  43: '\"Q\"',\n",
        "  44: '\"QA\"',\n",
        "  45: '\"QA\"',\n",
        "  46: '\"QA\"',\n",
        "  47: '\"R\"',\n",
        "  48: '\"RA\"',\n",
        "  49: '\"RB\"',\n",
        "  50: '\"S\"',\n",
        "  51: '\"SA\"',\n",
        "  52: '\"T\"',\n",
        "  53: '\"TB\"',\n",
        "  54: '\"TC\"',\n",
        "  55: '\"U\"',\n",
        "  56: '\"V\"',\n",
        "  57: '\"VB\"',\n",
        "  58: '\"VC\"',\n",
        "  59: '\"W\"',\n",
        "  60: '\"X\"',\n",
        "  61: '\"XA\"',\n",
        "  62: '\"Y\"',\n",
        "  63: '\"YA\"',\n",
        "  64: '\"YB\"',\n",
        "  65: '\"Z\"',\n",
        "  66: '\"ZA\"',\n",
        "  67: '\"ZB\"'},\n",
        " 3: {0: 531608.62,\n",
        "  1: 531189.27,\n",
        "  2: 531297.52,\n",
        "  3: 530361.04,\n",
        "  4: 530392.53,\n",
        "  5: 529279.64,\n",
        "  6: 529320.0,\n",
        "  7: 529339.0,\n",
        "  8: 529839.62,\n",
        "  9: 529696.37,\n",
        "  10: 529715.0,\n",
        "  11: 529661.0,\n",
        "  12: 531572.15,\n",
        "  13: 531320.28,\n",
        "  14: 530793.01,\n",
        "  15: 531036.02,\n",
        "  16: 531021.08,\n",
        "  17: 531558.11,\n",
        "  18: 530098.97,\n",
        "  19: 530718.43,\n",
        "  20: 530778.35,\n",
        "  21: 531099.49,\n",
        "  22: 531248.62,\n",
        "  23: 531287.44,\n",
        "  24: 530956.98,\n",
        "  25: 530771.84,\n",
        "  26: 530129.91,\n",
        "  27: 530009.18,\n",
        "  28: 530563.66,\n",
        "  29: 531786.91,\n",
        "  30: 532094.97,\n",
        "  31: 532187.4,\n",
        "  32: 529561.28,\n",
        "  33: 529091.72,\n",
        "  34: 529098.76,\n",
        "  35: 530560.41,\n",
        "  36: 529643.94,\n",
        "  37: 529236.05,\n",
        "  38: 529853.74,\n",
        "  39: 531076.26,\n",
        "  40: 530789.83,\n",
        "  41: 530371.07,\n",
        "  42: 530344.0,\n",
        "  43: 530361.0,\n",
        "  44: 530341.16,\n",
        "  45: 530333.0,\n",
        "  46: 530334.0,\n",
        "  47: 531260.77,\n",
        "  48: 531159.02,\n",
        "  49: 530790.38,\n",
        "  50: 531620.61,\n",
        "  51: 531327.96,\n",
        "  52: 529743.79,\n",
        "  53: 529249.66,\n",
        "  54: 529514.49,\n",
        "  55: 529683.69,\n",
        "  56: 531199.4,\n",
        "  57: 531661.06,\n",
        "  58: 531385.07,\n",
        "  59: 529654.6,\n",
        "  60: 530719.37,\n",
        "  61: 530832.27,\n",
        "  62: 529340.98,\n",
        "  63: 529680.55,\n",
        "  64: 529392.21,\n",
        "  65: 532820.52,\n",
        "  66: 532394.16,\n",
        "  67: 533147.52},\n",
        " 4: {0: 9907819.21,\n",
        "  1: 9907052.65,\n",
        "  2: 9907407.71,\n",
        "  3: 9905598.05,\n",
        "  4: 9905299.39,\n",
        "  5: 9906293.32,\n",
        "  6: 9906065.0,\n",
        "  7: 9905944.0,\n",
        "  8: 9905860.25,\n",
        "  9: 9906037.93,\n",
        "  10: 9906017.0,\n",
        "  11: 9906070.0,\n",
        "  12: 9909380.43,\n",
        "  13: 9909667.93,\n",
        "  14: 9906282.43,\n",
        "  15: 9906877.12,\n",
        "  16: 9906647.92,\n",
        "  17: 9908352.51,\n",
        "  18: 9908537.57,\n",
        "  19: 9905280.62,\n",
        "  20: 9904802.09,\n",
        "  21: 9903810.08,\n",
        "  22: 9904888.76,\n",
        "  23: 9905091.77,\n",
        "  24: 9907450.65,\n",
        "  25: 9907165.72,\n",
        "  26: 9907398.57,\n",
        "  27: 9907122.84,\n",
        "  28: 9904293.08,\n",
        "  29: 9908952.26,\n",
        "  30: 9906240.43,\n",
        "  31: 9906884.13,\n",
        "  32: 9904868.99,\n",
        "  33: 9905261.48,\n",
        "  34: 9904721.3,\n",
        "  35: 9906715.42,\n",
        "  36: 9903512.09,\n",
        "  37: 9907033.92,\n",
        "  38: 9906861.08,\n",
        "  39: 9909514.86,\n",
        "  40: 9910189.6,\n",
        "  41: 9906016.53,\n",
        "  42: 9905915.0,\n",
        "  43: 9906044.0,\n",
        "  44: 9905815.25,\n",
        "  45: 9905788.0,\n",
        "  46: 9905867.0,\n",
        "  47: 9907877.52,\n",
        "  48: 9907730.73,\n",
        "  49: 9907830.09,\n",
        "  50: 9908792.46,\n",
        "  51: 9909203.26,\n",
        "  52: 9908413.23,\n",
        "  53: 9907737.51,\n",
        "  54: 9909889.9,\n",
        "  55: 9902960.39,\n",
        "  56: 9909988.04,\n",
        "  57: 9911949.16,\n",
        "  58: 9910921.15,\n",
        "  59: 9904213.03,\n",
        "  60: 9905682.72,\n",
        "  61: 9906060.52,\n",
        "  62: 9905730.66,\n",
        "  63: 9905407.24,\n",
        "  64: 9905498.79,\n",
        "  65: 9909537.33,\n",
        "  66: 9911171.76,\n",
        "  67: 9908257.8}}"
      ],
      "metadata": {
        "id": "e2l71f6PfXS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cluster = pd.DataFrame(dict_cluster)"
      ],
      "metadata": {
        "id": "ulG18PqygRIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# adding well data component into dataframe"
      ],
      "metadata": {
        "id": "0QuLjp0cV8ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#accessing the made dataframe:\n",
        "group_dict = []\n",
        "for file in os.listdir(output_dir):\n",
        "  if file.endswith('.csv'):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "    name_file = file.split('.csv')[0]\n",
        "\n",
        "    #assign file name\n",
        "    displayed_dict = {}\n",
        "    displayed_dict[\"Filename\"]= name_file\n",
        "\n",
        "    #accessing column X and Y in df sumur in index 0 and last index for determining cluster\n",
        "    X = df.iloc[0][\"X\"]\n",
        "    Y = df.iloc[0][\"Y\"]\n",
        "    Z = df.iloc[0][\"Z\"]\n",
        "    X1 = df.iloc[-1][\"X\"]\n",
        "    Y1 = df.iloc[-1][\"Y\"]\n",
        "    Z1 = df.iloc[-1][\"Z\"]\n",
        "\n",
        "    displayed_dict[\"Cluster\"] = None\n",
        "\n",
        "    #create new col in df_cluster = \"DISTANCE TO CLUSTER\"\n",
        "    df_cluster[\"DISTANCE TO CLUSTER\"] = np.sqrt((df_cluster[3]-X)**2 + (df_cluster[4]-Y)**2)\n",
        "\n",
        "    #choose the min value in distance and assign the 0 column to be in displayed_dict[\"Cluster\"]\n",
        "    displayed_dict[\"Cluster\"] = df_cluster[df_cluster[\"DISTANCE TO CLUSTER\"] == df_cluster[\"DISTANCE TO CLUSTER\"].min()].iloc[0][0]\n",
        "\n",
        "    #assigning well head XY\n",
        "    displayed_dict[\"X\"] = X\n",
        "    displayed_dict[\"Y\"] = Y\n",
        "\n",
        "    #assigning departure\n",
        "    departure = np.sqrt((X1-X)**2 + (Y1-Y)**2 + (Z1-Z)**2)\n",
        "    displayed_dict[\"Departure\"] = round(departure, 0)\n",
        "\n",
        "    #assigning DLS\n",
        "    displayed_dict[\"highest DLS\"] = round(df[\"DLS\"].nlargest(1), 2).tolist()\n",
        "\n",
        "    #assigning inclination\n",
        "    displayed_dict[\"highest Inclination\"] = round(df[\"INCL\"].nlargest(1), 2).tolist()\n",
        "\n",
        "    #appending displayed dict to group dict\n",
        "    group_dict.append(displayed_dict)\n",
        "\n",
        "  else:\n",
        "      pass"
      ],
      "metadata": {
        "id": "S3dk3sngDK2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Surface"
      ],
      "metadata": {
        "id": "V1a3UsuhVAgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENTER SURFACE NAME AS THE NAME OF THE SURFACE FILE\n",
        "input_surface = \"R29-1\""
      ],
      "metadata": {
        "id": "dTi5LKEBK3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_surface = pd.read_csv(input_surface)\n",
        "\n",
        "#create dataframe into list\n",
        "data = df_surface.values.tolist()\n",
        "\n",
        "parted = []\n",
        "surface_list = []\n",
        "\n",
        "for item in data:\n",
        "  part = item[0].split()\n",
        "  parted.append(float(part[0]))\n",
        "  parted.append(float(part[1]))\n",
        "  parted.append(-(float((part[2]))))\n",
        "  surface_list.append(parted)\n",
        "  parted = []\n",
        "#change the item in surface list into a tuple\n",
        "\n",
        "surface_list_tuple = []\n",
        "for item in surface_list:\n",
        "  item = tuple(item)\n",
        "  surface_list_tuple.append(item)\n",
        "\n",
        "surface_list_tuple = np.array(surface_list_tuple)"
      ],
      "metadata": {
        "id": "uHinhSDcDciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COPY OF ABOVE CODE, JUST DELETE IF ONE IS OKE\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import re\n",
        "group_dict = []\n",
        "for file in os.listdir(output_dir):\n",
        "  if file.endswith('csv'):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "    name_file = file.split('csv')[0]\n",
        "\n",
        "    #assign file name\n",
        "    displayed_dict = {}\n",
        "    displayed_dict[\"Filename\"]= re.sub(r'\\.$', '', name_file)  #also deleting the dots in the end of filename.\n",
        "\n",
        "    #accessing column X and Y in df sumur in index 0 and last index for determining cluster\n",
        "    X = df.iloc[0][\"X\"]\n",
        "    Y = df.iloc[0][\"Y\"]\n",
        "    Z = df.iloc[0][\"Z\"]\n",
        "    X1 = df.iloc[-1][\"X\"]\n",
        "    Y1 = df.iloc[-1][\"Y\"]\n",
        "    Z1 = df.iloc[-1][\"Z\"]\n",
        "\n",
        "    displayed_dict[\"Cluster\"] = None\n",
        "\n",
        "    #create new col in df_cluster = \"DISTANCE TO CLUSTER\"\n",
        "    df_cluster[\"DISTANCE TO CLUSTER\"] = np.sqrt((df_cluster[3]-X)**2 + (df_cluster[4]-Y)**2)\n",
        "\n",
        "    #choose the min value in distance and assign the 0 column to be in displayed_dict[\"Cluster\"]\n",
        "    displayed_dict[\"Cluster\"] = df_cluster[df_cluster[\"DISTANCE TO CLUSTER\"] == df_cluster[\"DISTANCE TO CLUSTER\"].min()].iloc[0][0]\n",
        "\n",
        "    #assigning well head XY\n",
        "    displayed_dict[\"X\"] = X\n",
        "    displayed_dict[\"Y\"] = Y\n",
        "\n",
        "    #assigning departure\n",
        "    departure = np.sqrt((X1-X)**2 + (Y1-Y)**2 + (Z1-Z)**2)\n",
        "    displayed_dict[\"Departure\"] = round(departure, 0)\n",
        "\n",
        "    #assigning DLS\n",
        "    displayed_dict[\"highest DLS\"] = round(df[\"DLS\"].nlargest(1), 2).tolist()\n",
        "\n",
        "    #assigning inclination\n",
        "    displayed_dict[\"highest Inclination\"] = round(df[\"INCL\"].nlargest(1), 2).tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #take column xyz in df\n",
        "    df_xyz = df[[\"X\", \"Y\", \"Z\"]]\n",
        "\n",
        "    tuple_df_xyz = df_xyz.values.tolist()\n",
        "    tuple_well = []\n",
        "    for item in tuple_df_xyz:\n",
        "      item = tuple(item)\n",
        "      tuple_well.append(item)\n",
        "\n",
        "    tuple_well = np.array(tuple_well)\n",
        "\n",
        "    #lakukan KNN\n",
        "    k = 1\n",
        "    nn = NearestNeighbors(n_neighbors=k, algorithm = 'auto')\n",
        "\n",
        "    #fit model with the data\n",
        "    nn.fit(surface_list_tuple)\n",
        "\n",
        "    #find nearest neighbours\n",
        "    distances, indices = nn.kneighbors(tuple_well)\n",
        "\n",
        "    #take nearest neighbors\n",
        "    min_distance_index = np.argmin(distances)\n",
        "    nearest_neighbour = surface_list_tuple[indices[min_distance_index]]\n",
        "\n",
        "    '''for item in nearest_neighbour:\n",
        "      for value in item:\n",
        "        #INTO NON SCIENTIFIC FORMAT\n",
        "        value = round(value, 2)\n",
        "        displayed_dict[\"R29-1 COOR\"] = value'''\n",
        "\n",
        "    #create nested dict\n",
        "    displayed_dict[\"R29-1 COORDINATE\"] = []\n",
        "    for item in nearest_neighbour:\n",
        "      displayed_dict[\"R29-1 COORDINATE\"].append([round(value,2) for value in item])\n",
        "\n",
        "\n",
        "    #appending displayed dict to group dict\n",
        "    group_dict.append(displayed_dict)\n",
        "\n",
        "  else:\n",
        "      pass"
      ],
      "metadata": {
        "id": "dWCdRVQVkVAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "group_dict\n",
        "list_df = []\n",
        "for item in group_dict:\n",
        "  #make a dframe\n",
        "  df = pd.DataFrame(item)\n",
        "  list_df.append(df)\n",
        "\n",
        "#merge all df in the list\n",
        "df = pd.concat(list_df).reset_index(drop=True)\n",
        "# add delete \" \" and add -H di df[\"Cluster\"]\n",
        "df[\"Cluster\"] = df[\"Cluster\"].str.replace('\"', '')\n",
        "#add \"H-\" before the character\n",
        "df[\"Cluster\"] = \"H-\" + df[\"Cluster\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "OF9QKkGMnTxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFAYlzbCLE0N",
        "outputId": "3d16448d-a795-4c0a-ba3a-64ce1d30c985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Filename': 'HDZ-85 Rev1.1 KYL',\n",
              "  'Cluster': '\"D\"',\n",
              "  'X': 531562.0,\n",
              "  'Y': 9909393.0,\n",
              "  'Departure': 4083.0,\n",
              "  'highest DLS': [2.5],\n",
              "  'highest Inclination': [59.9],\n",
              "  'R29-1 COORDINATE': [[532680.84, 9911609.07, -3178.75]]},\n",
              " {'Filename': 'HDZ-93_Rev1.0_ABP WH LOC',\n",
              "  'Cluster': '\"D\"',\n",
              "  'X': 531562.0,\n",
              "  'Y': 9909393.0,\n",
              "  'Departure': 3128.0,\n",
              "  'highest DLS': [3.01],\n",
              "  'highest Inclination': [50.5],\n",
              "  'R29-1 COORDINATE': [[532080.84, 9909059.07, -2987.53]]},\n",
              " {'Filename': 'HDZ-87 Rev1.1 KYL',\n",
              "  'Cluster': '\"RA\"',\n",
              "  'X': 531158.0,\n",
              "  'Y': 9907729.0,\n",
              "  'Departure': 3177.0,\n",
              "  'highest DLS': [2.5],\n",
              "  'highest Inclination': [51.5],\n",
              "  'R29-1 COORDINATE': [[531530.84, 9906659.07, -2935.91]]}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DISPLAYING THE DATAFRAME RESULT**"
      ],
      "metadata": {
        "id": "C5b6X9QdaKqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VnFr1gMd0ApP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}